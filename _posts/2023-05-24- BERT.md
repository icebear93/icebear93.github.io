---
layout: single
title:  "Simple explanation of BERT"
categories: coding


author_profile: false
typora-root-url: ../
---

# What is BERT?

BERT (Bidirectional Encoder Representations from Transformers) is a natural language processing (NLP) model introduced by Google in 2018. It represents a significant advancement in the field of language understanding and has achieved state-of-the-art results in various NLP tasks.

BERT is based on the Transformer architecture, which is a neural network architecture that utilizes self-attention mechanisms to capture relationships between words in a sentence. Unlike previous NLP models that process text in a left-to-right or right-to-left manner, BERT introduces a bidirectional approach. It reads the entire input text in both directions, allowing the model to have a deeper understanding of the context and dependencies between words.

To get the idea what BERT is, it is important to understand the following concepts: 

* Sequence-to-Sequence
* Attention Mechanism
* Transformer 

---

## Sequence-to-Sequence

Seq2Seq, or sequence-to-sequence, is a framework used in natural language processing for tasks like language translation or text summarization. It involves two main parts: an encoder that processes the input sequence and creates a summary, and a decoder that takes the summary and generates the output sequence. It helps machines understand and generate sequences of data, like translating sentences from one language to another.

---

## Attention Mechanism

The attention mechanism is a component used in neural networks to focus on specific parts of input data. It helps models assign different weights or importance to different elements in a sequence, allowing them to selectively pay attention to relevant information. By doing so, attention mechanisms enable models to capture long-range dependencies and improve performance in tasks such as machine translation or text summarization.

---

## Transformer

: encoder and decoder made only by attention

The Transformer is a neural network architecture introduced in a paper titled "Attention is All You Need" in 2017. It revolutionized natural language processing tasks by leveraging self-attention mechanisms.

Unlike traditional recurrent neural networks (RNNs) that process sequential data step-by-step, the Transformer operates on the entire input sequence simultaneously. It consists of an encoder and a decoder, both composed of stacked layers.

The key innovation of the Transformer is the **self-attention mechanism**, which allows the model to weigh the importance of different words or tokens within a sequence based on their relationships. This attention mechanism enables the model to capture contextual dependencies effectively.

In the encoder, self-attention layers process the input sequence in parallel, capturing dependencies between all the words. The decoder uses masked self-attention, attending to previously generated words while predicting the next word in the sequence.

The Transformer architecture has several advantages. It reduces the reliance on sequential computation, making it highly parallelizable and efficient for training on GPUs. Additionally, it mitigates the vanishing gradient problem by using residual connections and layer normalization.

Transformers have achieved state-of-the-art results in various natural language processing tasks, including machine translation, text summarization, question answering, and sentiment analysis. Notable Transformer models include BERT, GPT, and T5.

---

## So what is BERT again?

BERT (Bidirectional Encoder Representations from Transformers) is a powerful language model developed by Google. It helps computers understand and generate human-like text. BERT is special because it considers the context of words in a sentence, allowing it to grasp the meaning of words based on their surroundings.

To understand a sentence, BERT reads it from both directions (left to right and right to left), capturing the relationships between words effectively. This bidirectional approach helps BERT understand the nuances and dependencies in language.

BERT goes through two main phases: pre-training and fine-tuning. During pre-training, BERT learns from a large amount of unlabeled text by predicting missing words in sentences. It becomes skilled at understanding the meaning and relationships between words.

In the fine-tuning phase, BERT is further trained on specific tasks like text classification, named entity recognition, or question answering. This fine-tuning process allows BERT to adapt its knowledge to the requirements of these tasks.

The remarkable thing about BERT is that it handles language intricacies such as word order, sentence structure, and multiple word meanings. It has achieved state-of-the-art performance in various language-related benchmarks and has become widely adopted in both academic research and practical applications.

---

## Reference

* [BERT explained in Korean](https://wikidocs.net/31379){: target ='_blank'}
* [klue BERT](https://huggingface.co/klue/bert-base){:target='_blank'}



