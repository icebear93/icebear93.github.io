---
layout: single
title:  "Simple explanation of BERT"
categories: md_test


author_profile: false
typora-root-url: ../
---

# What is BERT?



* sequence to sequence
* Attention mechanism
* Transformer : encoder and decoder made only by attention

## write anything

### hope it works

![bear](/images/2023-05-22-first/bear.PNG)

여기에 이 글을 추가로 첨가해봄. 어때?

